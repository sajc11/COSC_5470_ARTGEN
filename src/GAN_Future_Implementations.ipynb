{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqOj7Nyy8N8sedI+8mFCy9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oE8dQLxTkUlK"},"source":["# Real-Time Art Generation from Audio [Pathway 1]\n","This Google Colab notebook provides an interactive art generator that creates surrealist art in real time from recorded audio. The generated art is produced by a combination of a trained Convolutional Neural Network (CNN) model and a trained Generative Adversarial Network (GAN) generator, both pre-trained on datasets of spectrograms and surrealist art, respectively."]},{"cell_type":"markdown","source":["# Purpose & Future Implementation\n","Though this element of the code was not included in the final product of the project, due to time restraints, we hope to expand upon the following code in the future. The hope is that further development of the following code will allow us to integrate a real-time dynamic art generation feature to the current model of our audio-to-art generator."],"metadata":{"id":"Rx2gNUREUfdu"}},{"cell_type":"markdown","source":["# Integration for Future Development:\n","Advanced Audio Processing: Additional audio features,\n","\n","*   **User Feedback Loop:** Implementing a feedback mechanism where users can rate the generated art would allow the models to fine-tune outputs over time, achieving a dynamic, evolving art generator.\n","*   **Cross-Platform Integration:** The recording interface can be extended to integrate directly with streaming platforms or music applications, allowing for continuous, dynamic art generation based on a variety of audio inputs.\n","*   **Interactive Art Gallery:** Generated art can be archived and displayed in an interactive gallery, potentially integrating with web applications or augmented reality, enabling users to explore and engage with the generated images in new ways."],"metadata":{"id":"chVOOTiiU4Bz"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"q0lRF3z_Vd61"}},{"cell_type":"markdown","source":["# Loading Models:\n","The pre-trained CNN model is loaded from Google Drive, and can predict valence and energy values from a music spectrogram. The GAN generator is built using the `build_generator` function, synthesizing art from a latent vector."],"metadata":{"id":"V61lUbQ0VcIq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZYQ97uoufvC"},"outputs":[],"source":["# Load the trained models\n","cnn_model = tf.keras.models.load_model('/content/drive/My Drive/COSC_5470/trained_cnn_model.h5')\n","gan_generator = build_generator(latent_dim)  # Assuming this function is already defined"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjQXsRBNumVh"},"outputs":[],"source":["# Load the trained CNN model\n","cnn_model = tf.keras.models.load_model('/content/drive/My Drive/COSC_5470/trained_cnn_model.h5')\n","\n","# Load the trained GAN generator\n","gan_generator = tf.keras.models.load_model('/content/drive/My Drive/COSC_5470/saved_gan_model_final.h5')\n"]},{"cell_type":"markdown","source":["# Mapping Mood to Latent Space:\n","The function `map_mood_to_latent_space` converts the predicted valence and energy values from the CNN model into a latent vector, modulating it based on these values. The function scales both valence and energy to a range of [-1, 1], combining them into a modulation factor to create a corresponding latent vector."],"metadata":{"id":"EIwxSgZcVgZe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhx-A5Jouau0"},"outputs":[],"source":["def map_mood_to_latent_space(valence, energy, latent_dim=100):\n","    # Generate a base latent vector from a normal distribution\n","    latent_vector = np.random.normal(0, 1, (1, latent_dim))\n","\n","    # Modulate the latent vector based on valence and energy\n","    # Here we use an arbitrary way to combine valence and energy into the latent vector.\n","\n","    # Convert valence and energy into a range of [-1, 1] from [0, 1]\n","    valence = valence * 2 - 1\n","    energy = energy * 2 - 1\n","\n","    # Use valence and energy to modulate the latent vector\n","    modulation_factor = np.hstack([np.full((latent_dim // 2,), valence), np.full((latent_dim // 2,), energy)])\n","    modulated_latent_vector = latent_vector * modulation_factor\n","\n","    return modulated_latent_vector\n"]},{"cell_type":"markdown","source":["# Art Generation from Music:\n","The function `generate_art_from_music` uses the pre-trained CNN model to predict valence and energy values from a music spectrogram. These values are then passed to the `map_mood_to_latent_space` function to generate a modulated latent vector, which is fed into the GAN generator to synthesize the corresponding artwork."],"metadata":{"id":"CtO0vi13VrDs"}},{"cell_type":"code","source":["def generate_art_from_music(cnn_model, gan_generator, preprocessed_spectrogram):\n","    # Predict mood using the CNN model\n","    predictions = cnn_model.predict(np.expand_dims(preprocessed_spectrogram, axis=0))\n","    valence_energy = predictions[0]\n","\n","    valence = valence_energy[0]  # Assuming the first value is valence\n","    energy = valence_energy[1]   # Assuming the second value is energy\n","\n","    # Generate art using the latent vector that includes mood colors\n","    latent_vector = map_mood_to_latent_space(valence, energy)\n","    art = gan_generator.predict(np.array([latent_vector]))\n","\n","    return art.squeeze()"],"metadata":{"id":"xDzTh05qVpX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tu9pLikguinf"},"outputs":[],"source":["# Example usage with a path to a spectrogram image\n","spectrogram_file_path = '/content/drive/My Drive/COSC_5470/spectograms'\n","art = generate_art_from_music(cnn_model, gan_generator, spectrogram_file_path)"]},{"cell_type":"markdown","source":["# Real-Time Audio Recording and Processing:\n","The get_system_audio function records audio directly from the system using a virtual audio device, saving it as a flattened numpy array. The function `real_time_art_generation` then implements an interactive art generator: it initializes an image buffer to store audio spectrograms and uses rolling windows to update it with new audio snippets. It then converts the current buffer into a Mel spectrogram, which is preprocessed to match the CNN's input format. The mood is predicted from the spectrogram, a corresponding artwork is generated, and the Matplotlib plot is updated in real time."],"metadata":{"id":"kBodF19jV33_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVKJRNUmunzX"},"outputs":[],"source":["import librosa\n","import matplotlib.pyplot as plt\n","\n","def get_system_audio(duration, sr=22050):\n","    # Make sure the 'BlackHole' virtual audio device is installed and set as the input source\n","    recording = sd.rec(int(duration * sr), samplerate=sr, channels=1, dtype='float32')\n","    sd.wait()  # Wait until the recording is finished\n","    return recording.flatten()\n","\n","def real_time_art_generation(cnn_model, gan_generator, buffer_size, sr=22050, duration=5, overlap=0.5):\n","    plt.ion()  # Turn on interactive mode for dynamic updates\n","    fig, ax = plt.subplots()\n","    image = ax.imshow(np.random.rand(64, 64, 3), cmap='gray')  # Initial placeholder image\n","\n","    buffer = np.zeros(int(sr * duration * (1 + overlap)))  # Initialize buffer to hold extended duration for overlap\n","    hop_length = int(sr * duration * overlap)  # Define overlap size in samples\n","\n","    while True:\n","        try:\n","            # Simulate getting new audio snippet (this we will replace this with actual microphone input or other audio source)\n","            new_audio = get_system_audio(duration / 2, sr)  # duration is in seconds now\n","            buffer = np.roll(buffer, -hop_length)\n","            buffer[-hop_length:] = new_audio[-hop_length:]\n","\n","            # Convert audio snippet to a spectrogram\n","            spectrogram = librosa.feature.melspectrogram(current_audio, sr=sr, hop_length=hop_length, n_fft=2048)\n","            spectrogram = librosa.power_to_db(spectrogram, ref=np.max)  # Convert to dB scale for better visualization\n","\n","            # Preprocess the spectrogram (example: resizing to match input dimensions expected by the CNN)\n","            preprocessed_spectrogram = np.expand_dims(np.expand_dims(spectrogram, axis=-1), axis=0)\n","\n","            # Predict mood and generate art\n","            art = generate_art_from_music(cnn_model, gan_generator, preprocessed_spectrogram)\n","\n","            image.set_data(art)\n","\n","            # Update the plot\n","            image.set_data(art[0, :, :, :])  # Update the image with generated art\n","            fig.canvas.draw()\n","            fig.canvas.flush_events()\n","            # Assuming art is already in the correct format\n","            # ax.draw_artist(ax.patch)\n","            # ax.draw_artist(image)\n","\n","            plt.pause(0.1)  # Pause briefly to allow update\n","\n","        except KeyboardInterrupt:\n","           plt.ioff()\n","           break  # Exit on Ctrl+C\n","\n","# Define constants\n","# sr = 22050  # Sample rate\n","# duration = 5  # Duration of buffer in seconds\n","# overlap = 0.5  # 50% overlap\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"417PwUHEupFC"},"outputs":[],"source":["# Generate art from the new music piece\n","buffer_size = 22050 * 5  # = int(sr * duration * (1 + overlap))  # Buffer to hold extended duration for overlap\n","real_time_art_generation(cnn_model, gan_generator, buffer_size)"]},{"cell_type":"markdown","source":["# Integration for Future Development:\n","Improving audio processing by incorporating additional audio features, such as rhythm patterns or pitch, can further enrich the inputs, leading to more dynamic art. Web integration can allow the system to connect with streaming platforms, allowing for seamless art generation from a variety of audio sources. Optimizing performance and reducing latency can enhance real-time interactivity, enabling users to see the immediate effects of audio on art generation."],"metadata":{"id":"wuyv-DSOV-wU"}},{"cell_type":"markdown","metadata":{"id":"BZ1_CCZUjceg"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DRaYGXjdkHQv"},"source":["# Real-Time Art Generation from Music [Pathway 2]\n","This part of the Google Colab notebook demonstrates an additional pathway to the generation of surrealist art from audio inputs in real time. The implementation leverages a pre-trained Convolutional Neural Network (CNN) model to predict the mood (valence and energy) from a music spectrogram, which is then used to modulate a latent vector for a Generative Adversarial Network (GAN) generator to create corresponding artwork."]},{"cell_type":"markdown","source":["# Integration for Future Development:\n","* **Advanced Audio Processing:** Incorporating additional audio features such as rhythm patterns or pitch can further enrich the input data, leading to more dynamic art generation.\n","* **Web Integration:** The recording interface can be extended to integrate with streaming platforms or music applications, allowing for continuous, dynamic art generation from various audio sources.\n","* **Real-Time Performance:** Optimizing performance and reducing latency can enhance real-time interactivity, allowing users to see the immediate effects of audio on art generation."],"metadata":{"id":"xpQTlQyCXFfA"}},{"cell_type":"markdown","source":["# Dependencies Installation:\n","The necessary packages are installed, including TensorFlow, NumPy, Matplotlib, SciPy, FFmpeg-Python, and SoundDevice."],"metadata":{"id":"I2ymmhL-Wi3w"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1IEqPYYDYV3"},"outputs":[],"source":["!pip install IPython\n","!pip install numpy\n","!pip install matplotlib\n","!pip install scipy\n","!pip install ffmpeg-python\n","!pip install sounddevice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4JZ-hdD0eQD"},"outputs":[],"source":["import base64\n","import librosa\n","import matplotlib.pyplot as plt\n","import IPython\n","from IPython.display import Audio, display, HTML, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import numpy as np\n","from scipy.io.wavfile import read as wav_read\n","import io\n","import ffmpeg\n","from google.colab import output\n","\n"]},{"cell_type":"markdown","source":["# Loading Models:\n","Two pre-trained models are loaded from Google Drive:\n","\n","*   CNN Model: This model can predict valence and energy values from a music spectrogram.\n","*   GAN Generator: This model synthesizes surrealist art from a modulated latent vector."],"metadata":{"id":"Cy3YJcRSWnOj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qVkXZjbDYV3"},"outputs":[],"source":["# Load the trained CNN model\n","cnn_model = tf.keras.models.load_model('/content/drive/My Drive/COSC_5470/trained_cnn_model.h5')\n","\n","# Load the trained GAN generator\n","gan_generator = tf.keras.models.load_model('/content/drive/My Drive/COSC_5470/saved_gan_model_final.h5')\n"]},{"cell_type":"markdown","source":["# Audio Recording Interface:\n","An HTML interface is provided to record audio directly in the notebook. This interface allows users to start and stop recordings, save them as WebM files, and convert them to WAV format for further processing."],"metadata":{"id":"2vU-0503Wums"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lwt-Pxvt0e2V"},"outputs":[],"source":["AUDIO_HTML = \"\"\"\n","<div>\n","    <button onclick=\"startRecording()\">Start Recording</button>\n","    <button onclick=\"stopRecording()\" disabled>Stop Recording</button>\n","    <audio controls></audio>\n","    <script>\n","        let audioContext;\n","        let mediaRecorder;\n","        let audioChunks = [];\n","        let audioElement = document.querySelector('audio');\n","\n","        function startRecording() {\n","            audioChunks = [];\n","            navigator.mediaDevices.getUserMedia({ audio: true })\n","                .then(stream => {\n","                    audioContext = new AudioContext();\n","                    mediaRecorder = new MediaRecorder(stream);\n","                    mediaRecorder.start();\n","                    mediaRecorder.ondataavailable = event => {\n","                        audioChunks.push(event.data);\n","                    };\n","                    mediaRecorder.onstop = () => {\n","                        const audioBlob = new Blob(audioChunks, {type: 'audio/webm'});\n","                        const audioUrl = URL.createObjectURL(audioBlob);\n","                        audioElement.src = audioUrl;\n","                        const reader = new FileReader();\n","                        reader.readAsDataURL(audioBlob);\n","                        reader.onloadend = function() {\n","                            var base64data = reader.result;\n","                            document.getElementById('base64data').value = base64data;\n","                            google.colab.kernel.invokeFunction('notebook.get_audio_data', [base64data], {});\n","                        };\n","                    };\n","                    document.querySelector('button[onclick=\"stopRecording()\"]').disabled = false;\n","                    document.querySelector('button[onclick=\"startRecording()\"]').disabled = true;\n","                });\n","        }\n","\n","        function stopRecording() {\n","            mediaRecorder.stop();\n","            document.querySelector('button[onclick=\"startRecording()\"]').disabled = false;\n","            document.querySelector('button[onclick=\"stopRecording()\"]').disabled = true;\n","        }\n","    </script>\n","    <input type=\"hidden\" id=\"base64data\">\n","</div>\n","\"\"\""]},{"cell_type":"markdown","source":["# Audio Processing:\n","*   **Spectrogram Generation:** The WAV file is read and converted into a Mel spectrogram using the Librosa library, which serves as input to the art generator.\n","*   **Mood Prediction:** The spectrogram is preprocessed and passed to the CNN model, which predicts valence and energy values."],"metadata":{"id":"L39wLESeWz6e"}},{"cell_type":"code","source":["def get_audio_data(base64data):\n","    if ',' in base64data:\n","        binary = base64.b64decode(base64data.split(',')[1])\n","        with open(\"/content/temp_audio.webm\", \"wb\") as file:\n","            file.write(binary)\n","\n","        # Use ffmpeg to convert the webm file to wav format\n","        process = (ffmpeg\n","            .input('/content/temp_audio.webm')\n","            .output('/content/temp_audio.wav', format='wav')\n","            .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n","        )\n","        process.wait()  # Wait until the process is finished\n","\n","        # Read the WAV file\n","        sr, audio = wavfile.read('/content/temp_audio.wav')\n","        audio = audio.flatten()\n","        plt.figure(figsize=(20,10))\n","        plt.plot(audio)\n","        plt.title(\"Recorded Audio Waveform\")\n","        plt.show()\n","\n","        # Here you can add the code to generate spectrogram and art\n","        generate_art_from_audio(audio, sr)\n","    else:\n","        print(\"No audio data found. Please ensure the recording was made.\")\n","# Display the audio recording controls"],"metadata":{"id":"ywjoYxh-WyDH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Art Generation:\n","*   **Latent Vector:** The predicted mood values are used to modulate a latent vector, which is then fed into the GAN generator to create corresponding artwork.\n","*   **Real-Time Visualization:** The generated artwork is displayed in real time on a Matplotlib plot, which updates as new audio is recorded and processed."],"metadata":{"id":"H3XUV-J_W92V"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NcKXqQDVDYV4"},"outputs":[],"source":["def real_time_art_generation(cnn_model, gan_generator, duration=5, sr=22050, overlap=0.5):\n","    plt.ion()  # Interactive mode on\n","    fig, ax = plt.subplots()\n","    image = ax.imshow(np.random.rand(64, 64, 3), cmap='gray')\n","\n","    buffer_size = int(sr * duration * (1 + overlap))\n","    buffer = np.zeros(buffer_size)\n","\n","    hop_length = int(sr * duration * overlap)\n","    audio, sr = get_audio_data()\n","    while True:\n","        try:\n","            if audio is not None and sr is not None:\n","                buffer = np.roll(buffer, -len(audio))\n","                buffer[-len(audio):] = audio\n","\n","                # Convert audio snippet to a spectrogram\n","                spectrogram = librosa.feature.melspectrogram(buffer, sr=sr, hop_length=hop_length, n_fft=2048)\n","                spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n","\n","                # Preprocess and predict\n","                preprocessed_spectrogram = np.expand_dims(np.expand_dims(spectrogram, axis=-1), axis=0)\n","                art = generate_art_from_music(cnn_model, gan_generator, preprocessed_spectrogram)\n","\n","                # Update plot\n","                image.set_data(art[0, :, :, :])\n","                fig.canvas.draw()\n","                fig.canvas.flush_events()\n","\n","            plt.pause(0.1)  # Brief pause for updates\n","\n","        except KeyboardInterrupt:\n","            plt.ioff()\n","            break  # Exit on Ctrl+C\n","\n","# Define constants\n","# sr = 22050  # Sample rate\n","# duration = 5  # Duration of buffer in seconds\n","# overlap = 0.5  # 50% overlap\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTVA8yKjDYV4"},"outputs":[],"source":["# Generate art from the new music piece\n","buffer_size = 22050 * 5  # = int(sr * duration * (1 + overlap))  # Buffer to hold extended duration for overlap\n","real_time_art_generation(cnn_model, gan_generator, buffer_size)"]},{"cell_type":"markdown","metadata":{"id":"aTfE0TOT7W-l"},"source":["\n","\n","---\n","\n"]}]}